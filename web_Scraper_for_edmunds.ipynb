{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "import os\r\n",
                "from urllib.request import Request, urlopen\r\n",
                "from urllib.error import URLError\r\n",
                "import requests, wget, re\r\n",
                "from time import sleep\r\n",
                "import configuration as config\r\n",
                "from pathlib import Path\r\n",
                "\r\n",
                "from random_user_agent.user_agent import UserAgent\r\n",
                "from random_user_agent.params import SoftwareName, OperatingSystem\r\n",
                "\r\n",
                "# you can also import SoftwareEngine, HardwareType, SoftwareType, Popularity from random_user_agent.params\r\n",
                "# you can also set number of user agents required by providing `limit` as parameter\r\n",
                "\r\n",
                "software_names = [SoftwareName.CHROME.value]\r\n",
                "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]   \r\n",
                "\r\n",
                "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)\r\n",
                "\r\n",
                "# Get list of user agents.\r\n",
                "user_agents = user_agent_rotator.get_user_agents()\r\n",
                "\r\n",
                "# Get Random User Agent String.\r\n",
                "user_agent = user_agent_rotator.get_random_user_agent()\r\n",
                "#print(user_agent)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "from os.path import join\r\n",
                "\r\n",
                "webpage_dict = {}\r\n",
                "url_list = []\r\n",
                "\r\n",
                "download=False\r\n",
                "#get search urls / kbb only? \r\n",
                "with open(join(config.INPUT_DIR, 'url_list.txt'), 'r') as f:\r\n",
                "    search_url_list = f.readlines()\r\n",
                "\r\n",
                "#find urls in search pages\r\n",
                "for search_url in search_url_list:\r\n",
                "    base_url = '/'.join(search_url.split('/')[0:3])\r\n",
                "    if 'kbb' in base_url: \r\n",
                "        txt = requests.get(search_url).text\r\n",
                "        found_urls = re.findall(r'\"url\":\"(.*?)\"', txt)\r\n",
                "        found_urls = [x for x in found_urls if 'listingId' in x]\r\n",
                "        \r\n",
                "        if len(found_urls) == 0:\r\n",
                "            with open('debug.txt','w') as f:\r\n",
                "                print(\"Something's wrong\")\r\n",
                "                f.write(f\"{txt} \\n\")\r\n",
                "        print(f\"KBB total: {len(found_urls)}\")\r\n",
                "        url_list.extend(found_urls)\r\n",
                "\r\n",
                "\r\n",
                "    elif 'edmunds' in base_url:\r\n",
                "        headers = {\r\n",
                "                    'User-Agent': 'My User Agent 1.0',\r\n",
                "                    'From': 'youremail@domain.com'  # This is another valid field\r\n",
                "                }\r\n",
                "\r\n",
                "        txt = requests.get(search_url, headers=headers).text\r\n",
                "        found_urls = re.findall(r'\"url\":\"(.*?)\"', txt)\r\n",
                "\r\n",
                "        found_urls = [x for x in found_urls if 'vin' in x]\r\n",
                "        print(f\"Edmunds total: {len(found_urls)}\")\r\n",
                "        url_list.extend(found_urls)\r\n",
                "\r\n",
                "print(f\"Total: {len(url_list)}\")\r\n",
                "seen = set()\r\n",
                "uniq = []\r\n",
                "for x in url_list:\r\n",
                "    if x not in seen:\r\n",
                "        uniq.append(x)\r\n",
                "        seen.add(x)\r\n",
                "\r\n",
                "url_list = uniq\r\n",
                "print(f\"Grand total after duplicates removed: {len(url_list)}\")\r\n",
                "\r\n",
                "with open('urls_out.txt', 'w') as f:\r\n",
                "        for url in url_list:\r\n",
                "            f.write(f\"{url} \\n\")\r\n",
                "\r\n",
                "i=0 \r\n",
                "for url in url_list:\r\n",
                "    base_url = '/'.join(url.split('/')[0:3])\r\n",
                "    user_agent = 'My User Agent 1.0' if 'edmunds' in base_url else user_agent_rotator.get_random_user_agent()\r\n",
                "    print(f\"{i}\", end=\" \")\r\n",
                "    i+=1\r\n",
                "    req = Request(url, headers={'User-Agent':user_agent,\r\n",
                "                                'accept':'text/html;q=0.8,application/signed-exchange;v=b3;q=0.9',\r\n",
                "                                'accept-language':'en-US,en;q=0.9'})\r\n",
                "    try:\r\n",
                "        response = urlopen(req)\r\n",
                "    except URLError as e:\r\n",
                "        print(url)\r\n",
                "        if hasattr(e, 'reason'):\r\n",
                "            print('We failed to reach a server.')\r\n",
                "            print('Reason: ', e.reason)\r\n",
                "        elif hasattr(e, 'code'):\r\n",
                "            print('The server couldn\\'t fulfill the request')\r\n",
                "            print('Error code:', e.code)\r\n",
                "    else:\r\n",
                "        try:\r\n",
                "            if response.headers.get_content_charset() == 'utf-8':\r\n",
                "                webpage_as_string = response.read().decode(response.headers.get_content_charset(), \"ignore\")\r\n",
                "            else:\r\n",
                "                webpage_as_string = response.read().decode(response.headers.get_content_charset(), \"ignore\")\r\n",
                "        except UnicodeEncodeError as e:\r\n",
                "            print(e)\r\n",
                "            webpage_as_string = response.read()\r\n",
                "        #print(webpage_as_string)\r\n",
                "        #print(response.headers.get_content_charset())\r\n",
                "                \r\n",
                "        webpage_dict[url] = (webpage_as_string)\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Edmunds total: 16\n",
                        "2\n",
                        "Something's wrong\n",
                        "KBB total: 0\n",
                        "208\n",
                        "KBB total: 206\n",
                        "Total: 222\n",
                        "Grand total after duplicates removed: 108\n",
                        "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 https://www.kbb.com/cars-for-sale/vehicledetails.xhtml?listingId=589249917\n",
                        "We failed to reach a server.\n",
                        "Reason:  Forbidden\n",
                        "38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 https://www.kbb.com/cars-for-sale/vehicledetails.xhtml?listingId=586402136\n",
                        "We failed to reach a server.\n",
                        "Reason:  Forbidden\n",
                        "92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 "
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "import requests\r\n",
                "from ast import literal_eval\r\n",
                "import pandas as pd\r\n",
                "from os.path import join\r\n",
                "\r\n",
                "output_filename = join(config.OUTPUT_DIR, f\"potential vehicles.csv\")\r\n",
                "vehicle_df = pd.DataFrame(columns=[\"Price\", \"Mileage\", \"Year\", \"Make\", \"Model\", \"Trim\", \r\n",
                "                            \"Color\", \"Fuel Economy\", \"VIN\", \"Location\", \"URL\"])\r\n",
                "\r\n",
                "if os.path.exists(output_filename):\r\n",
                "    vehicle_df = pd.read_csv(output_filename)\r\n",
                "\r\n",
                "for url, txt in webpage_dict.items():\r\n",
                "    #print(url)\r\n",
                "    base_url = '/'.join(url.split('/')[0:3])\r\n",
                "\r\n",
                "        #kbb data scraper\r\n",
                "    if 'kbb' in base_url:  \r\n",
                "            #find vehicle data\r\n",
                "        vehicle_dict = {}\r\n",
                "        url = url.split(\"&\")[0]\r\n",
                "        try:\r\n",
                "            location_dict = literal_eval(re.findall(r'\"location\":({.*?})', txt)[1]+\"}\")['address']\r\n",
                "            #print(location_dict)\r\n",
                "            vehicle_data_dict = literal_eval(re.findall(r'\"vehicle\":({.*?}),', txt)[0].replace(\"null\",'\"N/A\"'))\r\n",
                "            #print(vehicle_data_dict)\r\n",
                "            pass\r\n",
                "        except Exception as e:\r\n",
                "            with open(\"debug.txt\", \"wb\") as f: f.write(txt.encode('utf-8'))\r\n",
                "            print(re.findall(r'\"location\":({.*?})', txt)[1]+\"}\")\r\n",
                "            print(re.findall(r'\"vehicle\":({.*?}),', txt)[0].replace(\"null\",'\"N/A\"'))\r\n",
                "            raise Exception\r\n",
                "            continue\r\n",
                "\r\n",
                "            #put vehicle data into dict\r\n",
                "        try:\r\n",
                "            address = location_dict['address1']\r\n",
                "        except:\r\n",
                "            address = \"N/A\"\r\n",
                "        city = location_dict['city']\r\n",
                "        state = location_dict['state']\r\n",
                "        zip_code = location_dict['zip']\r\n",
                "\r\n",
                "        full_address = f\"{address} {city}, {state} {zip_code}\"\r\n",
                "            \r\n",
                "        vehicle_dict[\"Price\"] = vehicle_data_dict['price']\r\n",
                "        vehicle_dict[\"Mileage\"] = int(\"\".join(vehicle_data_dict['odometer'].split(',')).split(\" \")[0])\r\n",
                "        vehicle_dict[\"Year\"] = vehicle_data_dict['car_year']\r\n",
                "        vehicle_dict[\"Make\"] = vehicle_data_dict['makeName'][0]\r\n",
                "        vehicle_dict[\"Model\"] = vehicle_data_dict['modelName'][0]\r\n",
                "        vehicle_dict[\"Color\"] = vehicle_data_dict['color'][0]\r\n",
                "        vehicle_dict[\"Fuel Economy\"] = \" | \".join(vehicle_data_dict['fuelEconomy']).replace(\"mpg \", \"\")\r\n",
                "        vehicle_dict[\"VIN\"] = vehicle_data_dict['vin']\r\n",
                "        vehicle_dict[\"Location\"] = full_address\r\n",
                "        vehicle_dict[\"URL\"] = url\r\n",
                "        \r\n",
                "        try:\r\n",
                "            vehicle_dict[\"Trim\"] = vehicle_data_dict['trim']\r\n",
                "        except:\r\n",
                "            pass\r\n",
                "\r\n",
                "        vehicle_df = vehicle_df.append(vehicle_dict, ignore_index=True)\r\n",
                "\r\n",
                "    elif 'edmunds' in base_url:\r\n",
                "            #find vehicle data\r\n",
                "        vehicle_dict = {}\r\n",
                "        price = re.findall(r'\"vdp-price-row\">(.*?)<', txt)[0][1:]\r\n",
                "        miles = int(re.findall(r'\"col\">(.*?) ', txt)[0].replace(\",\", \"\"))\r\n",
                "        year_make_model = re.findall(r'<h1 class=\"not-opaque text-black d-inline-block mb-0 size-24\">(.*?)</h1>', txt)[0].split(\" \")\r\n",
                "        year = year_make_model[0]\r\n",
                "        make = year_make_model[1]\r\n",
                "        model = year_make_model[2]\r\n",
                "        fuel_economy = re.findall(r'\"MPG\"></i></div><div class=\"col\">(.*?)<', txt)[0]\r\n",
                "        vin = url.split('/')[7]\r\n",
                "        location_dict = literal_eval(re.findall(r'\"location\":({.*?})', txt)[2].replace('false', \"False\"))\r\n",
                "        trim = \"N/A\"\r\n",
                "        try:\r\n",
                "            color = re.findall(r'<span>Ext: (.*?)<', txt)[0]\r\n",
                "        except Exception as e:\r\n",
                "            print(re.findall(r'<span>Ext: (.*?)<', txt))\r\n",
                "            color = \"N/A\"\r\n",
                "            print(e)\r\n",
                "            #raise Exception\r\n",
                "            continue\r\n",
                "\r\n",
                "            #put vehicle data into dict\r\n",
                "        latitude = location_dict['latitude']\r\n",
                "        longitude = location_dict['longitude']\r\n",
                "        \r\n",
                "        full_address = f\"{latitude} {longitude}\"\r\n",
                "            \r\n",
                "        vehicle_dict[\"Price\"] = price\r\n",
                "        vehicle_dict[\"Mileage\"] = miles\r\n",
                "        vehicle_dict[\"Year\"] = year\r\n",
                "        vehicle_dict[\"Make\"] = make\r\n",
                "        vehicle_dict[\"Model\"] = model\r\n",
                "        vehicle_dict[\"Trim\"] = trim\r\n",
                "        vehicle_dict[\"Color\"] = color\r\n",
                "        vehicle_dict[\"Fuel Economy\"] = fuel_economy.replace(\" / \", \" | \")\r\n",
                "        vehicle_dict[\"VIN\"] = vin\r\n",
                "        vehicle_dict[\"Location\"] = full_address\r\n",
                "        vehicle_dict[\"URL\"] = url\r\n",
                "\r\n",
                "        vehicle_df = vehicle_df.append(vehicle_dict, ignore_index=True)\r\n",
                "\r\n",
                "    elif '' in base_url:\r\n",
                "        pass\r\n",
                "        raise Exception(\"Stop here...\")\r\n",
                "\r\n",
                "    #add vehicle_dict to dataframe and save in csv\r\n",
                "print(f\"{len(vehicle_df)} cars found.\")\r\n",
                "vehicle_df = vehicle_df.drop_duplicates()\r\n",
                "print(f\"{len(vehicle_df)} after dupes removed\")\r\n",
                "vehicle_df.to_csv(join(output_filename), index=False)\r\n",
                "print(f\"Saved found cars to: {output_filename}\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[]\n",
                        "list index out of range\n",
                        "104 cars found.\n",
                        "104 after dupes removed\n",
                        "Saved found cars to: output\\potential vehicles.csv\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "source": [
                "import webbrowser\r\n",
                "for index, row in vehicle_df.iterrows():\r\n",
                "    print(f\"Row {index}: {row['Year']} {row['Make']} {row['Model']} for {row['Price']} with {row['Mileage']} miles\")\r\n",
                "    url = row[\"URL\"]\r\n",
                "    webbrowser.open(url, new=2)\r\n",
                "    rating = input(\"Input rating (1-10): \")\r\n",
                "    vehicle_df.at[index, 'My Rating'] = rating\r\n",
                "output_filename = output_filename = join(config.OUTPUT_DIR, f\"potential vehicles rated.csv\")\r\n",
                "vehicle_df.to_csv(join(output_filename), index=False)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Row 0: 2013 Toyota Corolla for 7,999 with 118189 miles\n",
                        "Row 1: 2015 Toyota Corolla for 8,995 with 152929 miles\n",
                        "Row 2: 2013 Toyota Corolla for 8,995 with 86647 miles\n",
                        "Row 3: 2010 Toyota Corolla for 7,000 with 153322 miles\n",
                        "Row 4: 2010 Toyota Corolla for 6,854 with 156170 miles\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.1",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.1 64-bit"
        },
        "interpreter": {
            "hash": "764ba89bf3c2408fcaa76b3d6f4b48249f73fb696a8c206ed5bda6c76d9498eb"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}